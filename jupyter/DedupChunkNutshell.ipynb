{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplication Chunking in a Nutshell\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Data deduplication is a technique for eliminating duplicate copies of repeating data. A related and somewhat synonymous term is single-instance (data) storage. (from: [dedup@wikipedia](https://en.wikipedia.org/wiki/Data_deduplication))\n",
    "\n",
    "This notebook will introduce chunking methods and demonstrate how it works. It is divided into two parts: part 1 - mini deduplication demo, part 2 - chunking methods demo.\n",
    "\n",
    "The example code is minimalistic. I wrote as little code as possible so anyone can play with it and understand some implications of chosing a chunking method and its algorithm's parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication in a Nutshell\n",
    "\n",
    "What deduplication does in a nutshell is:\n",
    "\n",
    "  1. Divide data chunks\n",
    "  2. Calculate the chunks' hashes\n",
    "  3. Store chunks uniquely\n",
    "  \n",
    "I will ignore the complexities of building such a system for actual big data, and write a simple key value storage class that saves so-called unique data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "class DeduplicationStorage:\n",
    "    def __init__(self, chunk_function):\n",
    "        self.chunk = chunk_function\n",
    "        self.hash = lambda raw, f=hashlib.md5: f(raw).digest()\n",
    "        self.chunks = {}\n",
    "        self.objects = {}\n",
    "\n",
    "    def write(self, key, value):\n",
    "        chunks = list(self.chunk(value))\n",
    "        hashes = [self.hash(raw) for raw in chunks]\n",
    "        self.chunks.update(zip(hashes, chunks))\n",
    "        self.objects[key] = hashes\n",
    "    \n",
    "    def read(self, key):\n",
    "        hashes = self.objects[key]\n",
    "        return b''.join(self.chunks[h] for h in hashes)\n",
    "        \n",
    "    def print_stats(self):\n",
    "        data_size = sum(len(self.chunks[h]) for hashes in self.objects.values() for h in hashes)\n",
    "        dedup_size = sum(len(raw) for raw in self.chunks.values())\n",
    "        print(f'Data Actual Size = {data_size} bytes.')\n",
    "        print(f'Deduplicated Size = {dedup_size} bytes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Methods\n",
    "\n",
    "Breifly chunking methods can be divided into 3 different categories:\n",
    "\n",
    "  1. Simple - for example fixed size.\n",
    "  2. Content aware - for example: files, objects.\n",
    "  3. Content sensitive - using a [rolling hash](https://en.wikipedia.org/wiki/Rolling_hash) to find uniquely defined boundaries.\n",
    " \n",
    "Below there is actual code for `chunking` data, and an axample section showing how it all fits together.\n",
    "\n",
    "### helper\n",
    "\n",
    "This will be used by all other functions to split the raw data besed on chunk boundary offsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_offsets(raw, offsets):\n",
    "    p = 0\n",
    "    for offset in offsets:\n",
    "        yield raw[p:offset]\n",
    "        p = offset\n",
    "    if p < len(raw):\n",
    "        yield raw[p:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Method - Simple\n",
    "The following function chunk the raw data into fixed size chunks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_size(raw, size=32):\n",
    "    offsets = range(size, len(raw), size)\n",
    "    return split_by_offsets(raw, offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Method - Content Aware\n",
    "The following function chunk the raw data into sentences, assuming the input raw data is a text. I used *text* and *sentences* since it is easy to reason about and inpect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def chunk_by_sentence(raw):\n",
    "    offsets = (match.end() for match in re.finditer(b'\\. ', raw))\n",
    "    return split_by_offsets(raw, offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Method - Content Sensetive\n",
    "The following function chunk the raw data into variable size chunks based on a hash value.\n",
    "Typical products will use rolling hash, but for this example I found no reason to complicate the code which is inefficient to begin with.\n",
    "\n",
    "The idea of hash based chunking, is to mark boudaries in areas which some random criteria applies on. In this way the boundaries of the same data will be in the same places even if the data is inserted into a different context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "WINDOW_SIZE = 4\n",
    "HASH = lambda raw: hashlib.md5(raw).digest()\n",
    "\n",
    "def chunk_by_hash(raw):\n",
    "    offsets = [\n",
    "        i + 1\n",
    "        for i in range(WINDOW_SIZE, len(raw))\n",
    "        if (HASH(raw[i - WINDOW_SIZE:i])[0] & 31) == 31\n",
    "    ]\n",
    "    return split_by_offsets(raw, offsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exmaple - How does it all fit together\n",
    "\n",
    "To get the feeling of how chunking works I will use the first paragraph from \"Phantom of the Opera\".\n",
    "I will store this paragraph in the demo-deduplication repository twice. The original version and a modified version changing the word \"absurd\" with \"joy\".\n",
    "\n",
    "What we will see in this example is the effect the chunking method have on a deduplication repository. We can then inspect the chunking methods to see why it effect the repository so greatly.\n",
    "\n",
    "### Defining Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = (b'THE Opera ghost really existed. He was not, as was long believed, a cr'\n",
    " b'eature of the imagination of the artists, the superstition of the managers, '\n",
    " b'or a product of the absurd and impressionable brains of the young ladies of '\n",
    " b'the ballet, their mothers, the box-keepers, the cloak-room attendants or the'\n",
    " b' concierge. Yes, he existed in flesh and blood, although he assumed the comp'\n",
    " b'lete appearance of a real phantom; that is to say, of a spectral shade.')\n",
    "\n",
    "modified = original.replace(b'absurd', b'joy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our \"Deduplication\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dedup test using chunk by size\n",
      "Data Actual Size = 887 bytes.\n",
      "Deduplicated Size = 727 bytes.\n",
      "\n",
      "Dedup test using chunk by sentence\n",
      "Data Actual Size = 887 bytes.\n",
      "Deduplicated Size = 720 bytes.\n",
      "\n",
      "Dedup test using chunk by hash\n",
      "Data Actual Size = 887 bytes.\n",
      "Deduplicated Size = 509 bytes.\n"
     ]
    }
   ],
   "source": [
    "for name, func in (\n",
    "    ('size', chunk_by_size),\n",
    "    ('sentence', chunk_by_sentence),\n",
    "    ('hash', chunk_by_hash),\n",
    "):\n",
    "    print(f'\\nDedup test using chunk by {name}')\n",
    "    dedup_store = DeduplicationStorage(func)\n",
    "\n",
    "    dedup_store.write('org', original)\n",
    "    dedup_store.write('mod', modified)\n",
    "\n",
    "    assert original == dedup_store.read('org')\n",
    "\n",
    "    dedup_store.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run 3 versions of this test. In each version we use a different chunking method and count the amount of actual data needed to be stored in the deduplicated so-called repository and the actual data size given by the user.\n",
    "\n",
    "I intentionally ignored any metadata and other artifacts. It is usually the case that the smaller the chunk size is the more metadata per any mount of user data grows. An actual deduplication system architecture is complicated and looks nothing like this example. Compacting the metadata is a different story I will not cover here.\n",
    "\n",
    "As you can see, in this case chunking yield significantly better data reduction results.\n",
    "It is the usual case when the deduplication system architect have no prior knowledge on the information the raw data represent and how it represents it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the chunking methods\n",
    "\n",
    "Now I would like to take a look at each one of those methods and see how the chunks look like. I defined a short that creates a short representation of a chunk in a human readable text. The original include short hash of the chunk, the size of the chunk, and the first 10 chrecters of the chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper short hash representation of chunks\n",
    "short = lambda x: f'{hex(hash(x))[-6:]} {len(x): 3d} {x.decode()[:10]}{\"...\" if len(x) > 10 else \"\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Size\n",
    "now lets take a look at the simple method of chunking by size, by inpsecting the original paragraph chunks and the modified paragraph's chunks with an indication for each chunk in the modified paragraph for if it can be found in the original paragraph after it was chunked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "fd180d  32 THE Opera ...\n",
      "f5986b  32 He was not...\n",
      "3f82df  32 , a creatu...\n",
      "faf507  32 of the art...\n",
      "9d76d9  32  of the ma...\n",
      "3162eb  32 f the absu...\n",
      "65b2f7  32 brains of ...\n",
      "622d66  32 e ballet, ...\n",
      "2fab5f  32 -keepers, ...\n",
      "15e51c  32 nts or the...\n",
      "9a3b5a  32 isted in f...\n",
      "9a5725  32 gh he assu...\n",
      "41d665  32 rance of a...\n",
      "1eab05  29  to say, o...\n",
      "dup fd180d  32 THE Opera ...\n",
      "dup f5986b  32 He was not...\n",
      "dup 3f82df  32 , a creatu...\n",
      "dup faf507  32 of the art...\n",
      "dup 9d76d9  32  of the ma...\n",
      "new 43ff5a  32 f the joy ...\n",
      "new eba3e0  32 ins of the...\n",
      "new 1e2c09  32 allet, the...\n",
      "new 6a0867  32 epers, the...\n",
      "new 9b2eba  32  or the co...\n",
      "new 34b50b  32 ed in fles...\n",
      "new 6e058f  32 he assumed...\n",
      "new 3150bc  32 ce of a re...\n",
      "new 1e6e71  26  say, of a...\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\")\n",
    "print('\\n'.join(short(chunk) for chunk in chunk_by_size(original)))\n",
    "isdup = lambda c, cs = set(chunk_by_size(original)): 'dup' if c in cs else 'new'\n",
    "print('\\n'.join(f'{isdup(chunk)} {short(chunk)}' for chunk in chunk_by_size(modified)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the chunks up to the point of the change can be found in both versions of the paragraph. However from the change and on, nothing is identified as duplication.\n",
    "\n",
    "This is because the replaced word is of different size, which shifted the entire text afterwards. The fixed size method is known to be inefficient for any data that can be shifted.\n",
    "\n",
    "In some cases the fixed size method works best. Some applications optimize for block storage, if those products are the owner of the data you want to deduplicate it would be wise to use the very same block size your application uses. In other cases, fixed size chunking is used due to other limitations or requirements from the designed deduplication system.\n",
    "\n",
    "### Content Aware\n",
    "\n",
    "I will demonstrate this awareness by using identifying sentences.\n",
    "Lets see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "fd180d  32 THE Opera ...\n",
      "1571d0  278 He was not...\n",
      "a99641  135 Yes, he ex...\n",
      "dup fd180d  32 THE Opera ...\n",
      "new 50c385  275 He was not...\n",
      "dup a99641  135 Yes, he ex...\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\")\n",
    "print('\\n'.join(short(chunk) for chunk in chunk_by_sentence(original)))\n",
    "isdup = lambda c, cs = set(chunk_by_sentence(original)): 'dup' if c in cs else 'new'\n",
    "print('\\n'.join(f'{isdup(chunk)} {short(chunk)}' for chunk in chunk_by_sentence(modified)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the only changed chunk was the chunk representing the sentence that was changed.\n",
    "This method is useful when the user of the deduplication system saved objects that can be identified as a whole and both thier size and changes are big enough to ignore partial changes or group it in case it is too small.\n",
    "\n",
    "### Content Sensitive\n",
    "\n",
    "This is the most robust method, since you need not to assume nothing about the input. In this example it works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "fd180d  32 THE Opera ...\n",
      "1571d0  278 He was not...\n",
      "a99641  135 Yes, he ex...\n",
      "dup fd180d  32 THE Opera ...\n",
      "new 50c385  275 He was not...\n",
      "dup a99641  135 Yes, he ex...\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\")\n",
    "print('\\n'.join(short(chunk) for chunk in chunk_by_sentence(original)))\n",
    "isdup = lambda c, cs = set(chunk_by_sentence(original)): 'dup' if c in cs else 'new'\n",
    "print('\\n'.join(f'{isdup(chunk)} {short(chunk)}' for chunk in chunk_by_sentence(modified)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example this method yields the smallest change.\n",
    "\n",
    "One intersting thing here is that there are two new chunks in the modified paragraph, but only one in the original. The word changed, as small as it may be, added a new boundary.\n",
    "\n",
    "considering other methods, when using fixed size it cannot happen, and when using content aware it can only happen when the semantics of the content are changed, in this example if we break a sentence in the middle adding a dot, a new boundary will appear.\n",
    "\n",
    "To play with the size of the chunk, you can try and play with the criteria. In this code we check if 5 bits are all 1. we can actually compare the to any number with same number of bits. So the average chunk size should be 2^5, so if we want larger chunk sizes we'll add bits, or remove bits, or redefine any criteria that suits our needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "I hope this will help you understand chunking methods, and the motivation of using hashes to chunk data for deduplication.\n",
    "\n",
    "Cheers,<br/>\n",
    "Johnny D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
