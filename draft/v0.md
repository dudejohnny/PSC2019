
# Fast SIMD-Based Chunking Algorithm

Paper Authors: Johnny Dude, Michael Hirsch, Yair Toaff
Conference: PSC 2019

## Outline
- Background
- Chunking Problem
- Traditional Solutions
- Our Solution
- Future Work

## Background – Deduplication

*Deduplication* is a technique for eliminating duplicate copies of repeating data.

### Deduplication process in a nutshell
- Divide into chunks
- Calculate the chunks' hashes
- Store chunks uniquely

### Example 
- Two objects
- Two objects chunked
- Example Repository
	
## Background - Chunking Methods

### How to chunk the input data
- Simple - fixed size.
- Content aware - files, objects, applications.
- Content sensitive - rolling hash.

### Example 
Show the first paragraph from “Phantom of the Opera” divided by:
- Size
- Sentences
- Hash
- Example Code (from the notebook)

## Background - Deduplication Performance

In 2017 we worked on a deduplication engine and we tried to improve its performance.

Show graph of seconds to process GB of the system parts:
 - 2.533 compressing with LZ4
- 1.120 chunking with Buzzhash
- 0.167 hashing with SHA-1
- 0.150 everything else

## Chunking Problem

Given a stream of bytes, divide it into chunks for deduplication.
- Output identical chunks for identical data</li>
- Good chunk size distribution.</li>
- Good performance.</li>
- Works for any input (photo, DB, text, random, etc...)</li>

## Traditional Solutions

- Karp-Rabin
- Cyclic Polynomial

### Example

- Hash and criteria
- Rolling hash
- Intrinsic operations comparison

## Proposed Solution

### How does it work

- Work with rolling vectors
- Calculate hash of byte size
- Calculate the criteria, in a way that:
  - Number of calculations are constant unrelated to the vector size.
  - Can find a cutting point at a byte offset.

### Example

- Hash
- Rolling hash
- Criteria
- Counting Zeroes

## Measured Results

- Size Distribution
- Algorithm Performance
- Dedup System Performance

## Future Work

What if the chunking algorithm was:
- Backward compatible
- Did the same amount of work
- Speed scaled with the vector size
